import os
import tiktoken
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_xai import ChatXAI

# .zshrc ã§è¨­å®šã•ã‚ŒãŸ GROK_API_KEY ã‚’å–å¾—
grok_api_key = os.getenv("XAI_API_KEY")
if grok_api_key is None:
    st.error("XAI_API_KEYãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

###### dotenv ã‚’åˆ©ç”¨ã—ãªã„å ´åˆã¯æ¶ˆã—ã¦ãã ã•ã„ ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################

# å„ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³å˜ä¾¡ï¼ˆä»®ã®å€¤ï¼‰
MODEL_PRICES = {
    "input": {
        "o1": 0.5 / 1_000_000,
        "gemini-2.0-flash": 3.5 / 1_000_000,
        "grok-2-latest": 0.7 / 1_000_000,
        "claude-3-7-sonnet-latest": 3 / 1_000_000
    },
    "output": {
        "o1": 1.5 / 1_000_000,
        "gemini-2.0-flash": 10.5 / 1_000_000,
        "grok-2-latest": 2.0 / 1_000_000,
        "claude-3-7-sonnet-latest": 15 / 1_000_000
    }
}


def init_page():
    st.set_page_config(
        page_title="My Great Chat-LLMs",
        page_icon="ğŸ¤—"
    )
    st.header("My Great Chat-LLMs ğŸ¤—")
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    # clear_button ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã‚„ message_history ãŒã¾ã å­˜åœ¨ã—ãªã„å ´åˆã«åˆæœŸåŒ–
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]


def select_model():
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§ temperature ã‚’é¸æŠ (0.0ï½2.0)
    temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.0, step=0.01)

    # æ–°ã—ã„4ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§é¸æŠ
    models = ("chatGPT o1", "Gemini 2.0 Flash", "grok-2", "Claude 3.7 Sonnet")
    model = st.sidebar.radio("Choose a model:", models)
    
    if model == "chatGPT o1":
        st.session_state.model_name = "o1"
        return ChatOpenAI(
            model_name=st.session_state.model_name
        )
    elif model == "Gemini 2.0 Flash":
        st.session_state.model_name = "gemini-2.0-flash"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name
        )
    elif model == "grok-2":
        st.session_state.model_name = "grok-2-latest"
        return ChatXAI(
            temperature=temperature,
            model_name=st.session_state.model_name
        )
    elif model == "Claude 3.7 Sonnet":
        st.session_state.model_name = "claude-3-7-sonnet-latest"
        return ChatAnthropic(
            temperature=temperature,
            model_name=st.session_state.model_name
        )


def init_chain():
    st.session_state.llm = select_model()
    prompt = ChatPromptTemplate.from_messages([
        *st.session_state.message_history,
        ("user", "{user_input}")  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒã“ã“ã«å…¥ã‚‹
    ])
    output_parser = StrOutputParser()
    return prompt | st.session_state.llm | output_parser

def get_message_counts(text):
    """
    ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦é©åˆ‡ãªæ–¹æ³•ã§ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã™ã€‚
    ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ•´æ•°ã§è¿”ã—ã¾ã™ã€‚
    """
    try:
        model_name = st.session_state.model_name.lower()
        
        # Geminiãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å†…è”µã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½¿ç”¨
        if "gemini" in model_name:
            return st.session_state.llm.get_num_tokens(text)
        
        # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€tiktoken ã‚’é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ä½¿ç”¨
        encoding_name = "cl100k_base"  # æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        if "gpt" in model_name or "o1" in model_name:
            # OpenAIãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            try:
                encoding = tiktoken.encoding_for_model(model_name)
            except KeyError:
                encoding = tiktoken.get_encoding(encoding_name)
        elif "grok" in model_name:
            # Grokã¯ cl100k_base ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
            encoding = tiktoken.get_encoding(encoding_name)
        else:
            # Claudeãªã©ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ cl100k_base ã‚’ä½¿ç”¨
            encoding = tiktoken.get_encoding(encoding_name)
            
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‹ã«å¿œã˜ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆ
        if isinstance(text, str):
            return len(encoding.encode(text))
        elif isinstance(text, list):
            count = 0
            for item in text:
                if isinstance(item, dict) and "content" in item:
                    count += len(encoding.encode(item["content"]))
                elif isinstance(item, str):
                    count += len(encoding.encode(item))
            return count
        return 0
    except Exception as e:
        st.warning(f"Token counting error: {e}")
        return 0  # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯0ã‚’è¿”ã—ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’é˜²æ­¢

def get_message_counts(message):
    # ... (çœç•¥) ...
    # ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾— (st.session_state.model_name ã« grok-2-latest ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æ)
    # model_name = st.session_state.model_name  # â† ã“ã®è¡Œã¯ä¸è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š (cl100k_base ã‚’ä¾‹ã¨ã—ã¦ä½¿ç”¨)
    encoding = tiktoken.get_encoding("cl100k_base")  # ã¾ãŸã¯é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å

    token_count = 0
    if isinstance(message, str):
        token_count = len(encoding.encode(message))
    elif isinstance(message, list):
        for item in message:
            if isinstance(item, dict) and "content" in item:
                token_count += len(encoding.encode(item["content"]))
    return token_count



def calc_and_display_costs():
    output_count = 0
    input_count = 0
    for role, message in st.session_state.message_history:
        token_count = get_message_counts(message)
        if role == "ai":
            output_count += token_count
        else:
            input_count += token_count

    # åˆæœŸçŠ¶æ…‹ï¼ˆSystem Message ã®ã¿ï¼‰ã®å ´åˆã¯ API ã‚³ãƒ¼ãƒ«ãªã—
    if len(st.session_state.message_history) == 1:
        return

    input_cost = MODEL_PRICES['input'][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES['output'][st.session_state.model_name] * output_count
    if "gemini" in st.session_state.model_name.lower() and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2

    cost = output_cost + input_cost

    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${cost:.5f}**")
    st.sidebar.markdown(f"- Input cost: ${input_cost:.5f}")
    st.sidebar.markdown(f"- Output cost: ${output_cost:.5f}")


def main():
    init_page()
    init_messages()
    chain = init_chain()

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å–å¾—
    if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
        st.chat_message('user').markdown(user_input)

        # LLM ã®è¿”ç­”ã‚’ Streaming è¡¨ç¤º
        with st.chat_message('ai'):
            response = st.write_stream(chain.stream({"user_input": user_input}))

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))

    # ã‚³ã‚¹ãƒˆã®è¨ˆç®—ã¨è¡¨ç¤º
    calc_and_display_costs()


if __name__ == '__main__':
    main()
