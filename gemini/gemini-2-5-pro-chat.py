import os
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import traceback

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    st.error("`langchain-google-genai` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

api_key = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))

if not api_key:
    st.error("GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloudã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

@st.cache_resource
def initialize_model(temperature, api_key):

    try:
        model = ChatGoogleGenerativeAI(
            model="gemini-2.5-pro-exp-03-25",
            temperature=temperature,
            google_api_key=api_key 
        )

        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ temperature={temperature} ã§åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
        return model
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.error("è€ƒãˆã‚‰ã‚Œã‚‹åŸå› : ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒãªã„ã€Google Cloudã®è¨­å®šä¸å‚™ãªã©ã€‚")
        st.stop()

def main():
    st.set_page_config(page_title="My Great Gemini 2.5 Pro", page_icon="ğŸ¤—")
    st.header("My Great Gemini 2.5 Pro ğŸ¤—")

    temperature = st.sidebar.slider(
        "Temperature:",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.01,
        help="å€¤ãŒé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ ã«ã€ä½ã„ã»ã©ä¸€è²«æ€§ã®ã‚ã‚‹å†…å®¹ã«ãªã‚Šã¾ã™ã€‚"
    )

    try:
        model = initialize_model(temperature, api_key)
    except Exception:
        return

    system_prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚"

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "system", "content": system_prompt}]

    for message in st.session_state.messages:
        if message["role"] != "system":
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    user_input = st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã­ï¼")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        langchain_messages = []
        for msg in st.session_state.messages:
            if msg["role"] == "system":
                langchain_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                langchain_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                langchain_messages.append(AIMessage(content=msg["content"]))

        with st.spinner("Gemini 2.5 Pro is thinking..."):
            try:
                response = model.invoke(langchain_messages)

                if isinstance(response, AIMessage):
                    response_text = response.content
                else:
                    st.error(f"äºˆæœŸã—ãªã„å¿œç­”å½¢å¼ã‚’å—ã‘å–ã‚Šã¾ã—ãŸ: {type(response)}")
                    response_text = "ã‚¨ãƒ©ãƒ¼: å¿œç­”ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚äºˆæœŸã—ãªã„å½¢å¼ã§ã™ã€‚"
                    st.error(f"å¿œç­”å†…å®¹: {response}")

                st.session_state.messages.append({"role": "assistant", "content": response_text})
                with st.chat_message("assistant"):
                    st.markdown(response_text)

            except Exception as e:
                st.error(f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.error("è©³ç´°æƒ…å ±:")
                st.error(traceback.format_exc())

if __name__ == "__main__":
    main()
