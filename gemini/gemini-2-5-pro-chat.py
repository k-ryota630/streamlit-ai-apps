import os
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    st.error("`langchain_openai` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- APIã‚­ãƒ¼ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ ---
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    if not api_key:
        st.error("GOOGLE_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloudã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

@st.cache_resource 
def initialize_model(temperature, api_key):
    try:
        model = ChatGoogleGenerativeAI(
            model_name="gemini-2.5-pro-exp-03-25",
            temperature=temperature,
            api_key=api_key
        )
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ temperature={temperature} ã§åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
        return model
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.stop()

def main():
    st.set_page_config(page_title="My Great Gemini 2.5 Pro", page_icon="ğŸ¤—")
    st.header("My Great Gemini 2.5 Pro ğŸ¤—")

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®šé …ç›® ---
    temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.7, step=0.01
    )

    model = initialize_model(temperature, api_key)

    system_prompt = "You are a helpful assistant."

    # --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç®¡ç† ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # --- å±¥æ­´ã®è¡¨ç¤º ---
    for message in st.session_state.messages:
        if message["role"] != "system":
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

    user_input = st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼")
    if user_input:
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã¨ç”»é¢ã«è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("Gemini 2.5 Pro is thinking..."):
            try:
                langchain_messages = []
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®å…ˆé ­ã«è¿½åŠ 
                langchain_messages.append(SystemMessage(content=system_prompt))

                for msg in st.session_state.messages:
                    if msg["role"] == "user":
                        langchain_messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        langchain_messages.append(AIMessage(content=msg["content"]))

                # --- Gemini 2.5 Pro LLMã®å‘¼ã³å‡ºã—
                response = model.invoke(langchain_messages)

                if isinstance(response, AIMessage):
                    response_text = response.content
                elif isinstance(response, str):
                    response_text = response
                elif hasattr(response, 'text'):
                    response_text = response.text
                else:
                    st.error(f"äºˆæœŸã—ãªã„å¿œç­”å½¢å¼ã§ã™: {type(response)}")
                    response_text = "ã‚¨ãƒ©ãƒ¼: å¿œç­”ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

                # å¿œç­”ã‚’å±¥æ­´ã¨ç”»é¢ã«è¿½åŠ  (roleã¯'assistant')
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                with st.chat_message("assistant"):
                    st.markdown(response_text)

            except Exception as e:
                st.error(f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.error(traceback.format_exc()) 

if __name__ == "__main__":
    main()
